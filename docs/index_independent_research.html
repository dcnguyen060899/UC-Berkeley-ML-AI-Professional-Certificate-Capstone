<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Duy Integral Theorem - Mathematical Research</title>
    <link rel="stylesheet" href="css/portfolio.css">
    <link rel="stylesheet" href="css/research.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <a href="https://em-executive.berkeley.edu/professional-certificate-machine-learning-artificial-intelligence" class="logo-link">
            <div class="logo">
                <img src="images/UC_Berkeley.png" alt="Berkeley Engineering and Haas logo">
            </div>
        </a>
        <nav>
            <a href="index_portfolio.html">Portfolio</a>
        </nav>
    </header>

    <div class="research-container">
        <div class="paper-header">
            <h1>Geometric Implicit Regularization: Duy Integral Theorem</h1>
            <div class="paper-meta">
                <p class="author">Duy Nguyen, B.A. (Economics)</p>
                <p class="affiliation">Working Paper - Independent Research</p>
                <p class="date">December 2024</p>
            </div>
            <div class="paper-links">
                <a href="https://drive.google.com/file/d/1K0lywYnA6MddzZtKFpsYuPjmBQva0apj/view?usp=sharing" class="download-button" target="_blank">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path><polyline points="7 10 12 15 17 10"></polyline><line x1="12" y1="15" x2="12" y2="3"></line></svg>
                    Download Full Paper (PDF)
                </a>
                <p class="citation">Nguyen, D. (2024). Geometric Implicit Regularization: Duy Integral Theorem. <em>Working Paper</em>.</p>
            </div>
        </div>

        <section id="abstract" class="abstract">
            <h2>Abstract</h2>
            <p>Deep neural networks, particularly in overparameterized regimes, exhibit remarkable learning and generalization capabilities that defy classical intuition. A leading hypothesis is that gradient-based training dynamics implicitly favor wide, flat minima in the loss landscape—regions often correlated with robust, well-generalizing solutions. However, translating this heuristic into a rigorous, unifying mathematical framework has proven challenging. <strong>Duy Integral Theory</strong> seeks to address this gap by treating the parameter space of a neural network as a continuum endowed with a time-evolving measure that flows under gradient descent. By defining an integral over submanifolds of "equivalent expressivity," this theory captures how measure concentrates on flat, generalizing regions while exponentially suppressing sharper, overfitting directions. In essence, Duy Integral Theory offers a measure-theoretic and PDE-based explanation for why overparameterized networks, despite their large dimensionality, converge to solutions that generalize effectively. This paper details the core ideas, the formal PDE framework, and the technical lemmas ensuring the existence and uniqueness of these measure evolutions. Our results illuminate the deeper geometric–measure-theoretic principles underlying deep learning's success and serve as a foundation for further theoretical and practical advances.</p>
        </section>

        <section id="theorem-statement" class="theorem-statement">
            <h2>1. Duy Integral Theorem: Statement</h2>
            <div class="theorem-block">
                <p>Let \(\mathcal{M}\subseteq \mathbb{R}^n\) be the parameter space of a (potentially overparameterized) neural network, equipped with a smooth loss function \(\mathcal{L}:\mathcal{M}\to\mathbb{R}\). Suppose \(\{P_i\}_{i\in I}\) is a partition of \(\mathcal{M}\) into submanifolds corresponding to equivalence classes of "similar expressivity." For each \(t\ge0\), let \(\mu_t\) be the time-evolving measure over \(\mathcal{M}\) determined by the gradient-flow continuity equation</p>

                <div class="equation">
                    \[ \begin{cases}
                    \displaystyle
                    \frac{\partial \mu_t}{\partial t}
                    \;+\;
                    \nabla_w \cdot 
                    \Bigl(\mu_t\bigl(-\nabla_w\mathcal{L}(w)\bigr)\Bigr)
                    \;=\;0,
                    \\[6pt]
                    \mu_{t=0} \;=\;\mu_0,
                    \end{cases} \]
                </div>

                <p>for some initial measure \(\mu_0\). Define the <strong>Duy Integral</strong> of a "neural-approximable" function \(f:\mathcal{M}\to\mathbb{R}\) as</p>

                <div class="equation">
                    \[ \int^D f
                    \;:=\;
                    \lim_{t\to\infty}
                    \;\lim_{n,L\to\infty}
                    \sum_{i\in I}
                    f(w_i)\;\mu_t(P_i), \]
                </div>

                <p>where \(w_i\in P_i\) is a chosen representative and \(\mu_t(P_i)\) is the measure of submanifold \(P_i\) at time \(t\).</p>
            </div>
        </section>

        <section id="main-theorem" class="main-theorem">
            <h2>2. Main Theorem</h2>
            <div class="theorem-block">
                <p><strong>Theorem (Duy Integral Theorem).</strong> Under suitable smoothness and regularity assumptions, the following holds:</p>
                <ol>
                    <li><strong>(Existence and uniqueness of \(\mu_t\))</strong> There is a unique measure solution \(\{\mu_t\}_{t\ge0}\) of the continuity equation.</li>
                    <li><strong>(Exponential suppression of sharp submanifolds)</strong> If a submanifold \(P_i\) exhibits strictly positive curvature (in the Hessian sense) along relevant directions, then \(\mu_t(P_i)\) decays exponentially to \(0\) as \(t\to\infty\).</li>
                    <li><strong>(Dominance of flat submanifolds)</strong> Submanifolds \(P_i\) with negligible or zero curvature retain non-vanishing measure.</li>
                    <li><strong>(Limit of the Duy Integral)</strong> Consequently, the Duy Integral \(\int^D f\) is determined entirely by contributions from those "flat" submanifolds, explaining why gradient descent in overparameterized neural networks converges to broad, generalizing solutions.</li>
                </ol>
            </div>
        </section>

        <section id="key-lemmas" class="key-lemmas">
            <h2>3. Key Mathematical Lemmas</h2>
            
            <div class="lemma">
                <h3>Lemma A.1 (Manifold Setup and Smoothness)</h3>
                <div class="lemma-content">
                    <p>Let \(\mathcal{M}\subseteq \mathbb{R}^n\) be an open subset or a smooth manifold (possibly with boundary). Suppose \(\mathcal{L}\in C^2(\mathcal{M})\) is a twice continuously differentiable function. Then for any compact subset \(K\subset\mathcal{M}\), there exists a Lipschitz constant \(L_K>0\) such that</p>
                    <div class="equation">
                        \[ \|\nabla_w \mathcal{L}(w_1) - \nabla_w \mathcal{L}(w_2)\|
                        \;\leq\;
                        L_K\|w_1 - w_2\|,
                        \quad
                        \forall w_1,w_2 \in K. \]
                    </div>
                </div>
            </div>
            
            <div class="lemma">
                <h3>Lemma B.1 (Existence of Measure Solutions)</h3>
                <div class="lemma-content">
                    <p>Let \(v(w)=-\nabla_w \mathcal{L}(w)\) be locally Lipschitz on \(\mathcal{M}\). Consider the continuity equation</p>
                    <div class="equation">
                        \[ \frac{\partial \mu_t}{\partial t} \;+\; \nabla_w\cdot\bigl(\mu_t\,v(w)\bigr) = 0,
                        \quad
                        \mu_{t=0} = \mu_0. \]
                    </div>
                    <p>Then there exists a unique solution \(\mu_t\) in the sense of measures (or distributions), for \(t\ge0\), given by</p>
                    <div class="equation">
                        \[ \mu_t = (\Phi_t)_*\mu_0 \]
                    </div>
                    <p>where \(\Phi_t\) is the flow map of the ODE \(\tfrac{dw}{dt}=v(w)\).</p>
                </div>
            </div>

            <div class="lemma">
                <h3>Proposition D.2 (Exponential Suppression of Sharp Submanifolds)</h3>
                <div class="lemma-content">
                    <p>Suppose submanifold \(P_i\) has \(\alpha_i>0\). Then there is a constant \(c_i>0\) such that</p>
                    <div class="equation">
                        \[ \mu_t(P_i) 
                        \;\le\;
                        \mu_0(P_i)\;\exp(-\,c_i\,t). \]
                    </div>
                    <p>Consequently, \(\lim_{t\to\infty}\mu_t(P_i)=0\).</p>
                </div>
            </div>
        </section>

        <section id="geometric-intuition" class="geometric-intuition">
            <h2>4. Geometric Intuition</h2>
            <p>The Duy Integral Theory provides a rigorous explanation for how gradient descent naturally discovers flat minima in the loss landscape. In high-dimensional parameter spaces, the model parameters are more accurately viewed as "flowing measures" rather than point particles following fixed paths.</p>
            
            <p>As gradient flow progresses, measure evacuation from sharply curved regions occurs exponentially fast—mirroring how probability mass concentrates on low energy states in statistical physics. The theorem proves that this evacuation isn't just a heuristic—it's a mathematical necessity embedded in the differential geometry of gradient flow.</p>
            
            <p>The key insight is that generalization in neural networks emerges naturally from this geometric flow property, without requiring explicit regularization. The "implicit bias" toward flat minima is formalized in the measure-theoretic framework, yielding a rigorous mathematical explanation for empirically observed phenomena like the ability of overparameterized networks to resist overfitting.</p>
            
            <p>What makes this framework particularly powerful is that it doesn't assume geometric properties from the outset. Instead, the preference for flat regions over sharp ones emerges naturally from the mathematics. This provides a principled explanation for why certain neural network configurations generalize better than others, grounded in the fundamental properties of measure evolution under gradient flow.</p>
        </section>

        <section id="visualizations" class="visualization">
            <h2>5. Visualizations</h2>
            <p>Consider a simple loss landscape with both sharp and flat minima. As training progresses, the measure (representing the parameter distribution) initially dispersed across the parameter space gradually concentrates on flat regions while evacuating sharp regions.</p>
            
            <div id="canvas-container">
                <div id="time-display">Time: t = 0</div>
            </div>
            
            <p>I've created an animated visualization that shows the continuous flow of measure from t=0 to t=∞, directly demonstrating the key concepts from the Duy Integral Theory paper.</p>
            
            <p>Features of the Visualization:</p>
            <ul>
                <li><strong>Continuous Animation:</strong>
                    <ul>
                        <li>Smoothly animates the measure flow from initial uniform distribution to final concentration</li>
                        <li>Time counter shows the progression from t=0 to t=10 (representing t=∞)</li>
                        <li>The animation cycles a few times for continuous observation</li>
                    </ul>
                </li>
                <li><strong>Realistic Measure Flow Dynamics:</strong>
                    <ul>
                        <li>Points near the sharp minimum evacuate more quickly (as predicted by the theory)</li>
                        <li>The animation shows gradual color transition (white → red/blue) based on region</li>
                        <li>Both position and color evolve to highlight the measure concentration process</li>
                    </ul>
                </li>
                <li><strong>Clear Geometric Features:</strong>
                    <ul>
                        <li>Distinct red sphere marking the sharp minimum</li>
                        <li>Distinct blue sphere marking the flat minimum</li>
                        <li>Detailed surface with wireframe showing the loss landscape</li>
                    </ul>
                </li>
            </ul>
            
            <p>The animation directly demonstrates how gradient descent naturally favors flat regions through the effect of geometry on measure flow. You can observe how initially the measure (points) is distributed uniformly, but over time the measure in sharp regions is exponentially suppressed while flat regions retain their measure, precisely as the Duy Integral Theorem predicts.</p>
            
            <p>This visual representation helps explain why seemingly counterintuitive practices like early stopping and small batch training can improve generalization—they align with the natural flow of measure toward flat, generalizing regions of the parameter space.</p>
            
            <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
            <script>
                const scene = new THREE.Scene();
                scene.background = new THREE.Color(0x000000);
                const camera = new THREE.PerspectiveCamera(60, window.innerWidth / window.innerHeight, 0.1, 1000);
                const renderer = new THREE.WebGLRenderer({ antialias: true });
                
                const container = document.getElementById('canvas-container');
                renderer.setSize(container.clientWidth, container.clientHeight);
                container.appendChild(renderer.domElement);
        
                // Time display element
                const timeDisplay = document.getElementById('time-display');
        
                // Add lights for better visibility
                const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
                scene.add(ambientLight);
                
                const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
                directionalLight.position.set(1, 2, 1);
                scene.add(directionalLight);
                
                const directionalLight2 = new THREE.DirectionalLight(0xffffff, 0.6);
                directionalLight2.position.set(-1, 1, -1);
                scene.add(directionalLight2);
        
                // Create loss landscape surface
                const size = 10;
                const resolution = 80;
                
                // Loss function with distinct sharp and flat minima
                function calculateLoss(x, z) {
                    // Sharp minimum at (-2.5, -2.5)
                    const sharpMin = 1.5 * Math.exp(-((x+2.5)**2 + (z+2.5)**2) * 3);
                    
                    // Flat minimum at (2.5, 2.5)
                    const flatMin = Math.exp(-((x-2.5)**2 + (z-2.5)**2) * 0.5);
                    
                    return 1 - Math.max(sharpMin, flatMin);
                }
        
                // Create surface for better visibility
                function createSurface() {
                    const geometry = new THREE.PlaneGeometry(size, size, resolution, resolution);
                    
                    // Deform surface based on loss
                    const positions = geometry.attributes.position.array;
                    for(let i = 0; i < positions.length; i += 3) {
                        const x = positions[i];
                        const z = positions[i + 2];
                        positions[i + 1] = calculateLoss(x, z) * 1.5;
                    }
                    
                    // Create material with transparency
                    const material = new THREE.MeshPhongMaterial({
                        color: 0xaaaaaa,
                        transparent: true,
                        opacity: 0.7,
                        side: THREE.DoubleSide,
                        wireframe: false,
                        flatShading: true
                    });
                    
                    const surface = new THREE.Mesh(geometry, material);
                    
                    // Add wireframe for better visibility
                    const wireframe = new THREE.LineSegments(
                        new THREE.WireframeGeometry(geometry),
                        new THREE.LineBasicMaterial({ color: 0x444444, linewidth: 1 })
                    );
                    
                    surface.add(wireframe);
                    scene.add(surface);
                    
                    return surface;
                }
                
                const surface = createSurface();
        
                // Create points that will animate over time
                const numPoints = 5000;
                const geometry = new THREE.BufferGeometry();
                const positions = new Float32Array(numPoints * 3);
                const colors = new Float32Array(numPoints * 3);
                const initialPositions = new Float32Array(numPoints * 3);
                const targetPositions = new Float32Array(numPoints * 3);
                
                // Initialize initial and target positions
                for(let i = 0; i < positions.length; i += 3) {
                    // Initial positions (t=0): uniform distribution
                    initialPositions[i] = (Math.random() - 0.5) * size;
                    initialPositions[i + 2] = (Math.random() - 0.5) * size; 
                    initialPositions[i + 1] = calculateLoss(initialPositions[i], initialPositions[i + 2]) * 1.5 + 0.1;
                    
                    // Target positions (t=∞): concentrated in flat minimum
                    const angle = Math.random() * Math.PI * 2;
                    const radius = Math.random() * 3;
                    
                    if(Math.random() < 0.95) {
                        // 95% of points in flat region
                        targetPositions[i] = 2.5 + radius * Math.cos(angle) * 0.7;
                        targetPositions[i + 2] = 2.5 + radius * Math.sin(angle) * 0.7;
                    } else {
                        // 5% random elsewhere
                        targetPositions[i] = (Math.random() - 0.5) * size;
                        targetPositions[i + 2] = (Math.random() - 0.5) * size;
                    }
                    
                    targetPositions[i + 1] = calculateLoss(targetPositions[i], targetPositions[i + 2]) * 1.5 + 0.1;
                    
                    // Start with initial positions
                    positions[i] = initialPositions[i];
                    positions[i + 1] = initialPositions[i + 1];
                    positions[i + 2] = initialPositions[i + 2];
                    
                    // Initial color: white
                    colors[i] = 1;     // R
                    colors[i + 1] = 1; // G
                    colors[i + 2] = 1; // B
                }
                
                geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
                geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));
                
                const material = new THREE.PointsMaterial({
                    size: 0.15,
                    vertexColors: true,
                    transparent: true,
                    opacity: 0.9
                });
                
                const points = new THREE.Points(geometry, material);
                scene.add(points);
        
                // Create visible markers for the minima
                function createSphereMarker(x, y, z, color, size) {
                    const geometry = new THREE.SphereGeometry(size, 16, 16);
                    const material = new THREE.MeshPhongMaterial({ color: color, transparent: true, opacity: 0.7 });
                    const sphere = new THREE.Mesh(geometry, material);
                    sphere.position.set(x, y, z);
                    scene.add(sphere);
                    return sphere;
                }
                
                // Add clear markers at minima
                const sharpMinima = createSphereMarker(-2.5, calculateLoss(-2.5, -2.5) * 1.5, -2.5, 0xff0000, 0.4);
                const flatMinima = createSphereMarker(2.5, calculateLoss(2.5, 2.5) * 1.5, 2.5, 0x0000ff, 0.4);
                
                // Create large, clear text labels
                function createTextSprite(text, fontSize, backgroundColor) {
                    const canvas = document.createElement('canvas');
                    canvas.width = 1024;
                    canvas.height = 512;
                    const context = canvas.getContext('2d');
                    
                    // Optional background for better readability
                    if (backgroundColor) {
                        context.fillStyle = backgroundColor;
                        context.fillRect(0, 0, canvas.width, canvas.height);
                    }
                    
                    context.font = `Bold ${fontSize || 60}px Arial`;
                    context.fillStyle = 'white';
                    context.textAlign = 'center';
                    context.textBaseline = 'middle';
                    context.fillText(text, canvas.width/2, canvas.height/2);
                    
                    const texture = new THREE.CanvasTexture(canvas);
                    const material = new THREE.SpriteMaterial({ map: texture, transparent: true });
                    const sprite = new THREE.Sprite(material);
                    sprite.scale.set(2, 1, 1);
                    return sprite;
                }
        
                // Add large, visible labels
                const sharpLabel = createTextSprite("Sharp Minimum", 48, "rgba(128,0,0,0.7)");
                sharpLabel.position.set(-2.5, 1.2, -2.5);
                scene.add(sharpLabel);
                
                const flatLabel = createTextSprite("Flat Minimum", 48, "rgba(0,0,128,0.7)");
                flatLabel.position.set(2.5, 1.2, 2.5);
                scene.add(flatLabel);
        
                // Add an infobox explaining the Duy Integral Theory
                const infoLabel = createTextSprite("Duy Integral Theory: Measure flows from sharp to flat regions", 36, "rgba(0,0,0,0.7)");
                infoLabel.position.set(0, 2, -5);
                scene.add(infoLabel);
        
                // Setup camera for better viewing
                camera.position.set(15, 10, 15);
                camera.lookAt(0, 0, 0);
        
                // Animation parameters
                let animationTime = 0;
                const animationDuration = 300; // Frames for full animation
                let isAnimating = true;
                let animationComplete = false;
                let cycleCount = 0;
                const totalCycles = 4;
        
                function animate() {
                    requestAnimationFrame(animate);
                    
                    // Rotation for camera
                    const cameraTime = Date.now() * 0.0001;
                    camera.position.x = 15 * Math.cos(cameraTime * 0.3);
                    camera.position.z = 15 * Math.sin(cameraTime * 0.3);
                    camera.lookAt(0, 0, 0);
                    
                    // Update point positions and colors based on animation time
                    if (isAnimating) {
                        animationTime++;
                        
                        // Normalized progress (0 to 1)
                        let progress = animationTime / animationDuration;
                        if (progress >= 1) {
                            if (cycleCount < totalCycles - 1) {
                                // Reset for next cycle
                                animationTime = 0;
                                cycleCount++;
                            } else {
                                // Hold at final state
                                progress = 1;
                                animationComplete = true;
                            }
                        }
                        
                        // Update time display
                        timeDisplay.textContent = `Time: t = ${progress.toFixed(2) * 10}`;
                        
                        // Custom easing for more realistic flow
                        // Slower at start, faster in middle, slower at end
                        const easedProgress = progress < 0.5 ? 
                            2 * progress * progress : 
                            1 - Math.pow(-2 * progress + 2, 2) / 2;
                        
                        const positions = points.geometry.attributes.position.array;
                        const colors = points.geometry.attributes.color.array;
                        
                        for(let i = 0; i < positions.length; i += 3) {
                            // For points initially near sharp minimum, have them leave earlier
                            const isNearSharp = Math.sqrt((initialPositions[i]+2.5)**2 + (initialPositions[i+2]+2.5)**2) < 2;
                            let pointProgress = isNearSharp ? 
                                Math.min(1, easedProgress * 1.5) : // Points near sharp region move faster
                                easedProgress;
                            
                            // If completing animation, force all to target
                            if (animationComplete) {
                                pointProgress = 1;
                            }
                            
                            // Intermediate positions
                            positions[i] = initialPositions[i] + (targetPositions[i] - initialPositions[i]) * pointProgress;
                            positions[i + 2] = initialPositions[i + 2] + (targetPositions[i + 2] - initialPositions[i + 2]) * pointProgress;
                            
                            // Update height to stay on surface
                            positions[i + 1] = calculateLoss(positions[i], positions[i + 2]) * 1.5 + 0.1;
                            
                            // Update color based on position (red for sharp, blue for flat)
                            const distToSharp = Math.sqrt((positions[i]+2.5)**2 + (positions[i+2]+2.5)**2);
                            const distToFlat = Math.sqrt((positions[i]-2.5)**2 + (positions[i+2]-2.5)**2);
                            
                            if (distToSharp < distToFlat) {
                                // Near sharp minimum: white → red
                                colors[i] = 1; // R
                                colors[i + 1] = 1 - pointProgress; // G
                                colors[i + 2] = 1 - pointProgress; // B
                            } else {
                                // Near flat minimum: white → blue
                                colors[i] = 1 - pointProgress; // R
                                colors[i + 1] = 1 - pointProgress; // G
                                colors[i + 2] = 1; // B
                            }
                        }
                        
                        points.geometry.attributes.position.needsUpdate = true;
                        points.geometry.attributes.color.needsUpdate = true;
                        
                        // Reset if animation complete and we've done all cycles
                        if (animationComplete && cycleCount >= totalCycles - 1) {
                            animationTime = 0;
                            cycleCount = 0;
                            animationComplete = false;
                        }
                    }
                    
                    // Update label orientations
                    sharpLabel.quaternion.copy(camera.quaternion);
                    flatLabel.quaternion.copy(camera.quaternion);
                    infoLabel.quaternion.copy(camera.quaternion);
                    
                    renderer.render(scene, camera);
                }
        
                animate();
        
                // Handle window resize
                window.addEventListener('resize', () => {
                    const width = container.clientWidth;
                    const height = container.clientHeight;
                    camera.aspect = width / height;
                    camera.updateProjectionMatrix();
                    renderer.setSize(width, height);
                });
            </script>
            
            <style>
                #canvas-container {
                    width: 100%;
                    height: 500px;
                    overflow: hidden;
                    background: #000;
                    margin: 20px 0;
                    position: relative;
                }
                canvas {
                    width: 100%;
                    height: 100%;
                }
                #time-display {
                    position: absolute;
                    top: 10px;
                    left: 10px;
                    color: white;
                    font-family: Arial, sans-serif;
                    font-size: 20px;
                    background-color: rgba(0,0,0,0.7);
                    padding: 5px 10px;
                    border-radius: 5px;
                }
            </style>
        </section>

        <section id="implications" class="implications">
            <h2>6. Implications and Applications</h2>
            <p>The Duy Integral Theorem has several important implications for deep learning theory and practice:</p>
            
            <ul>
                <li>It explains why stochastic gradient descent (SGD) outperforms deterministic gradient descent in many settings, as the stochasticity helps explore the geometric structure of flat manifolds.</li>
                <li>It provides theoretical justification for why large batch training can perform worse than small batch training, as large batches may impede the natural flow of measure through the parameter space.</li>
                <li>It suggests new optimization strategies that directly leverage geometric properties of the parameter space, potentially enabling faster convergence to generalizing solutions.</li>
                <li>It bridges the gap between the empirical success of overparameterized models and theoretical understanding of generalization, addressing a fundamental paradox in machine learning theory.</li>
            </ul>
        </section>
        
        <section id="conclusion" class="conclusion">
            <h2>7. Conclusion and Future Work</h2>
            <p>The Duy Integral Theorem brings a rigorous geometric and measure-theoretic perspective to understanding neural network optimization. By formulating gradient flow as a measure evolution process, it provides a solid mathematical foundation for explaining how neural networks naturally favor generalizing solutions despite their vast parameter spaces.</p>
            
            <p>This framework demonstrates that geometric properties of neural network training aren't imposed artificially but emerge naturally from the underlying mathematical principles governing gradient flow. The preference for flat minima isn't an assumed property but rather a derived consequence of how measures evolve in the parameter space.</p>
            
            <p>Future work includes extending the theory to stochastic gradient methods, developing practical algorithms that leverage these geometric insights, and exploring connections to other theoretical frameworks such as information geometry and optimal transport theory.</p>
        </section>

        <section id="references" class="references">
            <h2>References</h2>
            <div class="reference-list">
                <p class="reference">Nguyen, D. (2025). Geometric Implicit Regularization: Duy Integral Theorem. <em>Working Paper</em>. [<a href="https://drive.google.com/file/d/1K0lywYnA6MddzZtKFpsYuPjmBQva0apj/view?usp=sharing" target="_blank">PDF Link</a>]</p>
                <p class="reference">Ambrosio, L., Gigli, N., Savaré, G. (2008). <em>Gradient Flows</em>. Birkhäuser.</p>
                <p class="reference">Keskar, N.S., Mudigere, D., Nocedal, J., Smelyanskiy, M., Tang, P.T.P. (2017). <em>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</em>. ICLR.</p>
                <p class="reference">Dinh, L., Pascanu, R., Bengio, S., Bengio, Y. (2017). <em>Sharp Minima Can Generalize for Deep Nets</em>. ICML.</p>
            </div>
        </section>
    </div>

    <footer>
        <p>© 2024 UC Berkeley. All rights reserved.</p>
        <a href="index_portfolio.html">Back to Portfolio</a>
    </footer>
</body>
</html>
